{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 张量的创建\n",
    "\n",
    "-   张量（Tensors）类似于 Numpy 的 ndarrays，但张量可以在 GPU 上进行计算。所以从本质上来说，Pytorch 是一个处理张量的库。一个张量是一个数字、向量、矩阵或任何 n 维数组。\n",
    "-   下面分别展示了 0 维张量到 n 维张量：<br/>\n",
    "![tensor_example](https://gitee.com/Miraclezjy/utoolspic/raw/master/tensor-2022-1-2110:36:51.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x212657a8df0>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "torch.manual_seed(7)  # 固定随机数种子\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 一、直接创建\n",
    "\n",
    "-   `torch.tensor(data, dtype=None, device=None, requires_grad=False, pin_memory=False)`\n",
    "-   功能：从 data 创建 tensor\n",
    "    -   data：数据，可以是 list, numpy\n",
    "    -   dtype：数据类型，默认与 data 的一致\n",
    "    -   device：所在设备，cuda / cpu\n",
    "    -   requires_grad：是否需要梯度\n",
    "    -   pin_memory：是否存于锁页内存\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.1000, 1.2000],\n",
       "        [2.2000, 3.1000],\n",
       "        [4.9000, 5.2000]])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.tensor([[0.1, 1.2], [2.2, 3.1], [4.9, 5.2]])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-   `torch.from_numpy(ndarray)`\n",
    "-   功能：从 numpy 创建 tensor\n",
    "-   注意：从 torch.from_numpy 创建的 tensor 与原 ndarray 共享内存，当修改其中一个数据，另一个也将会被改动。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a:  [1 2 3]\n",
      "t:  tensor([1, 2, 3], dtype=torch.int32)\n"
     ]
    }
   ],
   "source": [
    "a = np.array([1, 2, 3])\n",
    "t = torch.from_numpy(a)\n",
    "print(\"a: \", a)\n",
    "print(\"t: \", t)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 二、依据数值创建\n",
    "\n",
    "1.  `torch.zeros(*size, out=None, dtype=None, layout=torch.strided, device=None, requires_grad=False)`\n",
    "\n",
    "-   功能：依 size 创建全 0 张量\n",
    "    -   size：张量的形状，如 (3, 3)、(3, 224, 224)\n",
    "    -   out：输出的张量\n",
    "    -   layout：内存中布局形式，有 strided, sparse_coo 等\n",
    "    -   device：所在设备，gpu / cpu\n",
    "    -   requires_grad：是否需要梯度\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0., 0.],\n",
       "        [0., 0., 0.]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.zeros(2, 3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. `torch.zeros_like(input, dtype=None, layout=None, device=None, requires_grad=False)`\n",
    "\n",
    "-   功能：依 input 形状创建全 0 张量\n",
    "    -   input：创建与 input 同形状的全 0 张量\n",
    "    -   dtype：数据类型\n",
    "    -   layout：内存中布局形式\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0., 0.],\n",
       "        [0., 0., 0.]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input = torch.empty(2, 3)\n",
    "torch.zeros_like(input)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. `torch.ones(*size, out=None, dtype=None, layout=torch.strided, device=None, requires_grad=False)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 1., 1.],\n",
       "        [1., 1., 1.]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.ones(2, 3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. `torch.ones_like(input, dtype=None, device=None, requires_grad=False)`\n",
    "\n",
    "-   功能：依 input 形状创建全 1 张量\n",
    "    -   size：张量的形状，如 (3, 3), (3, 224, 224)\n",
    "    -   dtype：数据类型\n",
    "    -   layout：内存中布局形式\n",
    "    -   device：所在设备，gpu / cpu\n",
    "    -   requires_grad：是否需要梯度\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 1., 1.],\n",
       "        [1., 1., 1.]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input = torch.empty(2, 3)\n",
    "torch.ones_like(input)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. `torch.full(*size, full_value, dtype=None, layout=torch.strided, device=None, requires_grad=False)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[3.1416, 3.1416, 3.1416],\n",
       "        [3.1416, 3.1416, 3.1416]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.full((2, 3), np.pi)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. `torch.full_like(input, dtype=None, layout=torch.strided, device=None, requires_grad=False)`\n",
    "\n",
    "-   功能：依 input 形状创建指定数据的张量\n",
    "    -   size：张量的形状，如 (3, 3)\n",
    "    -   fill_value：张量的值\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7. `torch.arange(start=0, end, step=1, out=None, dtype=None, layout=torch.strided, device=None, requires_grad=False)`\n",
    "\n",
    "-   功能：创建等差的 1 维向量\n",
    "    -   start：数列起始值\n",
    "    -   end：数列结束值\n",
    "    -   step：数列公差，默认为 1\n",
    "-   注意：数值区间为 `[start, end)`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1.0000, 1.5000, 2.0000])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.arange(1, 2.5, 0.5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "8. `torch.linspace(start, end, steps=100, out=None, dtype=None, layout=torch.strided, device=None, requires_grad=False)`\n",
    "\n",
    "-   功能：创建均分的 1 维张量\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-10.,  -5.,   0.,   5.,  10.])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.linspace(start=-10, end=10, steps=5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "9. `torch.logspace(start, end, steps=100, base=10.0, out=None, dtype=None, layout=torch.strided, device=None, requires_grad=False)`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 1.2589,  2.1135,  3.5481,  5.9566, 10.0000])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.logspace(start=0.1, end=1.0, steps=5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "10. `torch.eye(n, m=None, dtype=None, layout=torch.strided, device=None, requires_grad=False)`\n",
    "\n",
    "-   功能：创建单位对焦矩阵（2 维张量）\n",
    "-   注意：默认为方阵\n",
    "    -   n：矩阵行数\n",
    "    -   m：矩阵列数\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 0., 0.],\n",
       "        [0., 1., 0.],\n",
       "        [0., 0., 1.]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.eye(3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 三、依概率分布创建张量\n",
    "\n",
    "1. `torch.normal(mean, std, out=None)`\n",
    "\n",
    "-   功能：生成正态分布（高斯分布）\n",
    "\n",
    "    -   mean：均值\n",
    "    -   std：标准差\n",
    "\n",
    "-   四种模式：\n",
    "    -   mean 为标量，std 为标量\n",
    "    -   mean 为标量，std 为张量\n",
    "    -   mean 为张量，std 为标量\n",
    "    -   mean 为张量，std 为张量\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.8532, 2.7075, 3.7575, 3.2200, 6.0145, 5.5526, 6.8577, 8.3697, 9.0276,\n",
       "        9.8318])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# mean 为张量，std 为张量\n",
    "torch.normal(mean=torch.arange(1., 11.), std=torch.arange(1, 0, -0.1))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. `torch.normal(mean, std, size, out=None)`\n",
    "\n",
    "-   功能：生成一定大小的正态分布（高斯分布）\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[2.9530, 2.3984, 2.4120, 2.7216]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.normal(2, 3, size=(1, 4))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. `torch.randn(*size, out=None, dtype=None, layout=torch.strided, device=None, requires_grad=False)`\n",
    "\n",
    "-   功能：生成标准正态分布\n",
    "    -   size：张量的形状\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.3955, 1.3470, 2.4382],\n",
       "        [0.2028, 2.4505, 2.0256]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.randn(2, 3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. `torch.rand(*size, out=None, dtype=None, layout=torch.strided, device=None, requires_grad=False)`\n",
    "\n",
    "-   功能：在区间 `[0, 1)` 上，生成均匀分布\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.7405, 0.2529, 0.2332],\n",
       "        [0.9314, 0.9575, 0.5575]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.rand(2, 3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. `torch.randint(low=0, high, size, out=None, dtype=None, layout=torch.strided, device=None, requires_grad=False)`\n",
    "\n",
    "-   功能：在区间 `[low, high)` 生成整数均匀分布\n",
    "    -   size：张量的形状\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[4, 8],\n",
       "        [7, 8]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.randint(3, 10, (2, 2))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. `torch.randperm(n, out=None, dtype=torch.int64, layout=torch.strided, device=None, requires_grad=False)`\n",
    "\n",
    "-   功能：生成从 0 - 1 的随机排列\n",
    "    -   n：张量的长度\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 3, 0, 2])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.randperm(4)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7. `torch.bernoulli(input, *, generator=None, out=None)`\n",
    "\n",
    "-   功能：以 input 为概率，生成伯努利分布（0-1 分布，两点分布）\n",
    "    -   input：概率值\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a:  tensor([[0.4703, 0.1049, 0.5137],\n",
      "        [0.2674, 0.4990, 0.7447],\n",
      "        [0.7213, 0.4414, 0.5550]])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0., 1.],\n",
       "        [0., 1., 1.],\n",
       "        [1., 0., 1.]])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.empty(3, 3).uniform_(0, 1)\n",
    "print(\"a: \", a)\n",
    "torch.bernoulli(a)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 张量的操作\n",
    "\n",
    "## 一、张量拼接与切分\n",
    "\n",
    "1. `torch.cat(tensor, dim=0, out=None)`\n",
    "\n",
    "-   功能：将张量按维度 dim 进行拼接\n",
    "    -   tensors：张量序列\n",
    "    -   dim：要拼接的维度\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-1.7038,  0.6248,  0.1196, -1.7038,  0.6248,  0.1196, -1.7038,  0.6248,\n",
       "          0.1196],\n",
       "        [-0.8049,  1.6162,  0.2516, -0.8049,  1.6162,  0.2516, -0.8049,  1.6162,\n",
       "          0.2516]])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.randn(2, 3)\n",
    "torch.cat((x, x, x), 1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. `torch.stack(tensors, dim=0, out=None)`\n",
    "\n",
    "-   功能：在新创建的维度 dim 上进行拼接\n",
    "    -   tensors：张量序列\n",
    "    -   dim：要拼接的维度\n",
    "\n",
    "3. `torch.chunk(input, chunks, dim=0)`\n",
    "\n",
    "-   功能：将张量按维度 dim 进行平均切分\n",
    "-   返回值：张量列表\n",
    "-   注意：若不能整除，最后一份张量小于其他张量\n",
    "    -   input：要切分的张量\n",
    "    -   chunks：要切分的份数\n",
    "    -   dim：要切分的维度\n",
    "\n",
    "4. `torch.split(tensor, split_size_or_sections, dim=0)`\n",
    "\n",
    "-   功能：将张量按维度 dim 进行切分\n",
    "-   返回值：张量列表\n",
    "    -   tensor：要切分的张量\n",
    "    -   split_size_or_sections：为 int 时，表示每一份的长度；为 list 时，按 list 元素切分\n",
    "    -   dim：要切分的维度\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a:\n",
      " tensor([[0, 1],\n",
      "        [2, 3],\n",
      "        [4, 5],\n",
      "        [6, 7],\n",
      "        [8, 9]])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor([[0, 1],\n",
       "         [2, 3]]),\n",
       " tensor([[4, 5],\n",
       "         [6, 7]]),\n",
       " tensor([[8, 9]]))"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.arange(10).reshape(5, -1)\n",
    "print(\"a:\\n\", a)\n",
    "torch.split(a, 2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 二、张量索引\n",
    "\n",
    "1. `torch.index_select(input, dim, index, out=None)`\n",
    "\n",
    "-   功能：在维度 dim 上，按 index 索引数据\n",
    "-   返回值：依 index 索引数据拼接的张量\n",
    "    -   input：要索引的张量\n",
    "    -   dim：要索引的维度\n",
    "    -   index：要索引数据的序号\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.0510,  0.1323,  0.3916,  1.0830],\n",
       "        [ 0.3809,  0.2569, -1.0273,  0.4999]])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.randn(3, 4)\n",
    "indices = torch.tensor([0, 2])\n",
    "torch.index_select(x, 0, indices)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. `torch.masked_select(input, mask, out=None)`\n",
    "\n",
    "-   功能：按 mask 中的 True 进行索引\n",
    "-   返回值：一维张量\n",
    "    -   input：要索引的张量\n",
    "    -   mask：与 input 同形状的布尔类型张量\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.5054, 0.8079])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.randn(3, 4)\n",
    "mask = x.ge(0.5)\n",
    "torch.masked_select(x, mask)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 三、张量变换\n",
    "\n",
    "1. `torch.reshape(input, shape)`\n",
    "\n",
    "-   功能：变换张量形状\n",
    "-   注意：当张量在内存中是连续时，新张量与 input 共享数据内存\n",
    "    -   input：要变换的张量\n",
    "    -   shape：新张量的形状\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 1.],\n",
       "        [2., 3.]])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.arange(4.)\n",
    "torch.reshape(a, (2, 2))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. `torch.transpose(input, dim0, dim1)`\n",
    "\n",
    "-   功能：交换张量的两个维度\n",
    "    -   input：要交换的张量\n",
    "    -   dim0：要交换的维度\n",
    "    -   dim1：要交换的维度\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x:\n",
      " tensor([[ 0.0096, -0.5704, -0.1722],\n",
      "        [ 0.4506,  0.4101,  0.8957]])\n",
      "After transpose:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0096,  0.4506],\n",
       "        [-0.5704,  0.4101],\n",
       "        [-0.1722,  0.8957]])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.randn(2, 3)\n",
    "print(\"x:\\n\", x)\n",
    "print(\"After transpose:\")\n",
    "torch.transpose(x, 0, 1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. `torch.t(input)`\n",
    "\n",
    "-   功能：二维张量转置，对矩阵而言，等价于 `torch.transpose(input, 0, 1)`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x:\n",
      " tensor(-0.2882)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor(-0.2882)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.randn(())\n",
    "print(\"x:\\n\", x)\n",
    "torch.t(x)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. `torch.squeeze(input, dim=None, out=None)`\n",
    "\n",
    "-   功能：压缩长度为 1 的维度（轴）\n",
    "    -   dim：若为 None，移除所有长度为 1 的轴；若指定维度，当且仅当该轴长度为 1 时，可以被移除\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x:\n",
      " tensor([[[[[0., 0.]],\n",
      "\n",
      "          [[0., 0.]]]],\n",
      "\n",
      "\n",
      "\n",
      "        [[[[0., 0.]],\n",
      "\n",
      "          [[0., 0.]]]]])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[0., 0.],\n",
       "         [0., 0.]],\n",
       "\n",
       "        [[0., 0.],\n",
       "         [0., 0.]]])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.zeros(2, 1, 2, 1, 2)\n",
    "print(\"x:\\n\", x)\n",
    "y = torch.squeeze(x)\n",
    "y\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. `torch.unsqueeze(input, dim, out=None)`\n",
    "\n",
    "-   功能：依据 dim 扩展维度\n",
    "    -   dim：扩展的维度\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 2, 3, 4]])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.tensor([1, 2, 3, 4])\n",
    "torch.unsqueeze(x, 0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 线性回归模型\n",
    "\n",
    "线性回归是分析一个变量与另外一（多）个变量之间关系的方法。<br/>\n",
    "因变量是 y，自变量是 x，关系线性：$y = \\omega \\times x + b$，任务是求解 $\\omega, b$。<br/>\n",
    "我们的求解步骤是：\n",
    "\n",
    "1. 确定模型：$Model \\rightarrow y = \\omega \\times x + b$\n",
    "2. 选择损失函数：这里用 $MSE: \\frac{1}{m}\\sum_{i=1}^m(y_i-\\hat y_i)^2$\n",
    "3. 求解梯度并更新 $\\omega, b$：\n",
    "\n",
    "$$\n",
    "\\begin{array}{lcl}\n",
    "w &=& w - lr \\times w.grad \\\\\n",
    "b &=& b - lr \\times w.grad\n",
    "\\end{array}\n",
    "$$\n",
    "\n",
    "下面我们开始写一个线性回归模型：\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-1.3531e+27]) tensor([-2.1245e+26])\n"
     ]
    }
   ],
   "source": [
    "# 首先我们需要训练样本 X, Y，这里我们随机生成\n",
    "x = torch.rand(20, 1) * 10\n",
    "y = 2 * x + (5 + torch.randn(20, 1))\n",
    "\n",
    "# 构建线性回归函数的参数\n",
    "w = torch.randn((1), requires_grad=True)\n",
    "b = torch.randn((1), requires_grad=True)  # 这两者都需要求梯度\n",
    "\n",
    "# 设置学习率 lr 为 0.1\n",
    "lr = 0.1\n",
    "\n",
    "for iteration in range(100):\n",
    "    # 前向传播\n",
    "    wx = torch.mul(w, x)\n",
    "    y_pred = torch.add(wx, b)\n",
    "\n",
    "    # 计算 loss\n",
    "    loss = (0.5 * (y - y_pred)**2).mean()\n",
    "\n",
    "    # 反向传播\n",
    "    loss.backward()\n",
    "\n",
    "    # 更新参数\n",
    "    b.data.sub_(lr * b.grad)  # 这种 _ 的加法操作时从自身减，相当于 -=s\n",
    "    w.data.sub_(lr * w.grad)\n",
    "\n",
    "    # 梯度清零\n",
    "    w.grad.data.zero_()\n",
    "    b.grad.data.zero_()\n",
    "\n",
    "print(w.data, b.data)\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "3c55c1ad30c0bbb9f76ce180e5170039d3011e6a3fbd602a8e44c4bb4d29f3d1"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit ('base': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
